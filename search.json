[
  {
    "objectID": "qc.html",
    "href": "qc.html",
    "title": "Quality Control of Sequencing Data",
    "section": "",
    "text": "By the end of this module, you should be able to:",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#from-plant-tissue-to-sequencing-reads",
    "href": "qc.html#from-plant-tissue-to-sequencing-reads",
    "title": "Quality Control of Sequencing Data",
    "section": "From plant tissue to sequencing reads",
    "text": "From plant tissue to sequencing reads\nBefore we talk about quality control, it is important to understand how sequencing data is produced. Many of the problems we detect during QC originate early in the sequencing workflow.\nAt a high level, the process looks like this:\n\nPlant tissue → DNA extraction → Library preparation → Sequencing → FASTQ files\n\nEach step can introduce technical effects that later appear in quality control reports. Let’s have a closer look:\n\n\n\n\nWe start with plant tissue and extract the contained DNA.\n\n\n\n\nNext, sequencing machines do not read whole chromosomes. Instead, DNA is broken into many small fragments, the so called DNA library.\n\n\n\n\nThe DNA library (with adapters added), is then run on a sequencing machine, which calls the bases of each fragment.\n\n\n\n\nThe final output are usually FASTQ files, which contain raw reads and quality scores.",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#why-do-we-need-quality-control",
    "href": "qc.html#why-do-we-need-quality-control",
    "title": "Quality Control of Sequencing Data",
    "section": "Why do we need quality control?",
    "text": "Why do we need quality control?\nWhen we work with sequencing data, we usually want to answer a biological question.\nIn plant breeding, these could for example be:\n\nWhich genetic variants are associated with a certain trait?\nWhich alleles are present in a breeding line?\nHow similar are two plant populations?\nWhich genes are expressed under certain conditions?\n\nAll of these questions rely on the assumption that the sequencing data reflects the real biology of the plant.\nQuality control (QC) is the step where we check whether this assumption is reasonable.\nAs such, QC is not about making the data “look nice”. It is about making sure that technical problems do not lead to wrong biological conclusions.",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#what-can-go-wrong-if-data-quality-is-poor",
    "href": "qc.html#what-can-go-wrong-if-data-quality-is-poor",
    "title": "Quality Control of Sequencing Data",
    "section": "What can go wrong if data quality is poor?",
    "text": "What can go wrong if data quality is poor?\nSequencing errors do not stay “technical”. They can directly affect how we interpret the biology:\n\n\n\n\n\n\n\nTechnical issue\nBiological consequence\n\n\n\n\nLow base quality\nIdentification of false SNPs\n\n\nAdapter contamination\nDetection of artificial variants\n\n\nGC bias\nSkewed allele frequency estimates\n\n\nHigh duplication\nOverconfidence in genotypes\n\n\n\n In a breeding context, this can mean that we might choose the wrong markers, miss true marker-trait associations or misjudgement or genetiv diversity.\nQuality control helps us detect these problems before we trust the results.",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#where-do-sequencing-errors-come-from",
    "href": "qc.html#where-do-sequencing-errors-come-from",
    "title": "Quality Control of Sequencing Data",
    "section": "Where do sequencing errors come from?",
    "text": "Where do sequencing errors come from?\nSequencing errors can be introduced at several steps, long before we ever see a FASTQ file.\n\nStep 1: DNA extraction\n\nPlant material can be challenging to work with. Different issuea can affect the quality of the reads that will be sequenced:\n\nCompounds such as polyphenols and polysaccharides can interfere with enzymes or make the DNA sticky.\nDNA quality can vary strongly between samples, depending on the extraction method, tissue and extractor.\n\n\n\nStep 2: Library preparation\n\nDuring library preparation, the DNA is sheared into fragments, adapters are ligated to these DNA fragments and the DNA is often amplified using PCR.\nSometimes, the DNA already degrades before the library preparation. That can lead to uneven sequencing coverage in the sequenced reads. The PCR can introduce a bias, where some fragments are amplified more than others.\nAll these effects are technical, but they shape the data we later analyse.\n\n\nSequencing\n\nSequencing run itself can introduce errors. Base calling becomes less accurate toward the end of reads and errors accumulate across sequencing cycles. Different lanes or runs can have different overall quality.",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#why-plant-genomes-are-special",
    "href": "qc.html#why-plant-genomes-are-special",
    "title": "Quality Control of Sequencing Data",
    "section": "Why plant genomes are special",
    "text": "Why plant genomes are special\nMany QC tools are generic. They do not “know” that your data comes from plants.\nHowever, plant genomes often have features that affect quality:\n\nLarge genome sizes\nHigh repeat content\nPolyploidy\nHigh heterozygosity\nVariable GC content between species\n\nAs a result, the GC content plots may look unusual even for good data. High duplication levels are not always a technical problem. So, some warnings are expected and acceptable.\nThis means that QC results must always be interpreted in a biological context.\nQuality control tools can show you what looks unusual — you need biological knowledge to decide why.",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#what-is-fastqc",
    "href": "qc.html#what-is-fastqc",
    "title": "Quality Control of Sequencing Data",
    "section": "What is FastQC?",
    "text": "What is FastQC?\nAfter sequencing, the first files we usually receive are FASTQ files. These files contain millions of short sequencing reads together with a quality score for every base.\nLooking directly at raw FASTQ files is not practical.\nFastQC is a tool for quality control on raw sequence data coming from high throughput sequencing. It takes your raw data and performs a series of standardized tests, and produces an easy-to-read overview report with figures and tables for each sample.\nFastQC provides information about your samples, it does not know what experiment you performed, or what you plan to do with your data afterwards. Thus, the interpretation and decisions are up to you. After checking the quality you might want to use other tools to correct errors, remove low-quality data, etc.",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#interpreting-fastqc-flags",
    "href": "qc.html#interpreting-fastqc-flags",
    "title": "Quality Control of Sequencing Data",
    "section": "Interpreting FastQC flags",
    "text": "Interpreting FastQC flags\nEach FastQC module ends with a simple label: pass, warning, or fail. These labels are often misunderstood. A fail does not mean the data is unusable. Alternatively, a pass does not mean the data is perfect.\nThe flags are based on generic thresholds that work reasonably well across many datasets, but they are not tailored to specific organisms or analyses. In plant breeding projects, it is common to see warnings or failures even in perfectly usable datasets.\nFastQC flags should therefore be treated as signals to investigate, not as automatic decisions.",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#a-fastqc-report",
    "href": "qc.html#a-fastqc-report",
    "title": "Quality Control of Sequencing Data",
    "section": "A FastQC report",
    "text": "A FastQC report\nThe following is adapted from the excellent FastQC tutorial at Babraham Bioinformatics.\n\nBasic statistics\n\nSimple summary statistics for the file that was analyzed. Good to check what sample was used, and to get initial stats on the sample. This section will never raise warnings.\n\n\nPer base sequence quality\n\nThis plot shows the quality score at each position in the read. The higher the score, the better the base call.\nThis plot answers a simple question: How confident are we in the base calls along the read?\nIn most sequencing datasets, quality is highest at the beginning of the read and decreases toward the end. This pattern is normal and expected, a modest drop in quality at the end of reads is usually not a serious problem.\nHowever, the sample can be problematic when:\n\nQuality drops very sharply.\nLarge parts of the read have very low scores.\nQuality varies strongly between samples.\n\n\n\nPer Sequence Quality Scores\n\nWhile the previous plot looks at quality per position, this plot looks at quality per read.\nIt allows you to see if a subset of your sequences have low quality values, or if the quality is homogeneous across the reads of your sample.\nThe low quality reads should obviously only represent a small percentage of the total sequences. If a significant proportion of the sequences are of low quality there could be some kind of systematic problem.\nIn plant breeding data, poor per-sequence quality can indicate issues such as:\n\nProblems during library preparation.\nTechnical issues during sequencing.\nLow-quality starting material.\n\n\n\nPer Base Sequence Content\n\nThis module shows the proportion of the bases - A, C, G, and T - at each position in the read. For random genomic DNA, these proportions should be relatively stable across the read length.\nSome types of library will always produce biased sequence composition, normally at the start of the read. Not every deviation from equal base content is a problem. The key question is whether the pattern is expected given the experiment.\nIn plant genomes, GC content can vary naturally between regions, and this can influence this plot.\n\n\nPer Sequence GC Content\n\nThe GC content plot shows the overall GC distribution of reads in the sample. FastQC compares the observed distribution to a theoretical expectation, and can help identify contamination.\nThe expectation is a roughly normal distribution of GC content where the peak represents the overall GC content of the underlying genome. However, the acutal shape of the curve depends highly on your sample.\nIn plant datasets, unusual GC content does not automatically mean contamination.\nIt can reflect:\n\nGenome composition\nRepetitive regions\nTargeted sequencing approaches\n\nThis plot becomes more informative when comparing samples to each other, rather than judging a single sample in isolation.\n\n\nPer Base N Content\n\nIf a sequencer is unable to make a base call with sufficient confidence it will call an “N” rather than a conventional base.\nIt is not unusual to see a very low proportion of Ns appearing in a sequence, especially closer to the end of a sequence.\n\n\nSequence length distribution\n\nA graph showing the distribution of fragment sizes in the file which was analysed. Here, you can see a sample with many short, and a few very long reads.\nSome high throughput sequencers generate sequence fragments of uniform length, but others can contain reads of varying lengths.\n\n\nSequence Duplication Levels\n\nThis plot shows how often identical sequences occur in the data. In a diverse library most sequences will occur only once in the final set, so most sequences should fall into the far left of the plot\nHigh duplication can arise from:\n\nPCR amplification bias.\nLow library complexity.\nVery deep sequencing.\n\nIn plant breeding data, duplication is not always technical. Highly conserved or repetitive regions can also produce high duplication levels.\nInterpretation depends strongly on the type of experiment and the genome being studied.\n\n\nOverrepresented sequences\n\nFastQC also identifies sequences that occur much more often than expected. A normal high-throughput library will contain a diverse set of sequences, with no individual sequence making up a substantial fraction of the whole.\nOverrepresented sequences might be biologically significant, or indicate that the library is contaminated, or simply not as diverse as expected. They can be adapter fragments, primers or highly repetitive genomic regions.\nThis module is often useful for diagnosing specific technical issues, especially when combined with adapter content and duplication plots.\n\n\nKmer content\n\n\n\nAdapter content\n\nThis module will detect adapter content and plot it together with the identity of the adapter.\nGood to know if you have adapter contamination, and which ones you might have to remove from the data.",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "data_management.html#data-life-cycle",
    "href": "data_management.html#data-life-cycle",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Data Life Cycle",
    "text": "Data Life Cycle\nWhen working with any type of data, it makes sense to sit down before the project starts to think through the different life stages of the data in your project. This will help counteract some of the problems that can arise when projects grow more organically, and will help consistency within the research group, ease collaboration, and mostly your future self that will understand what past-self has been up to in the project.\n\n\n\n\n\n\nNote\n\n\n\nMore and more funding agencies expect a Data Management Plan at some point of a project application. In there, you need to document that you have thought of, and planned for, the life cycle of your data.\n\n\n\n\n\nThe Research Data Management toolkit for Life Sciences",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#fair-principles",
    "href": "data_management.html#fair-principles",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "FAIR principles",
    "text": "FAIR principles\nIn the past, research data was often generated with one question in mind. Often, they would afterwards land in some drawer and be forgotten about. Nowadays researchers acknowledge that data can also be re-used, or combined with other data, to answer different questions.\nThe FAIR principles promote efficient data discovery and reuse by providing guidelines to make digital resources:\n\n\n\nWilkinson et al. (2016)\n\n\nFAIR principles, in turn, rely on good data management practices in all phases of research:\n\nResearch documentation\nData organisation\nInformation security\nEthics and legislation",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#reproducible-research",
    "href": "data_management.html#reproducible-research",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Reproducible research",
    "text": "Reproducible research\nLucky for us, once we implement good data management practices, we will also increase the reproducibility of our analyses. Extensive documentation will increase faith in the outcome of analyses, and will help people (again, future-you) understand what has been done.\nLast, but not least, reproducible research practices make project hand-overs smoother, when the next person already understands the structure of the project, and can rely on good documentation.",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#what-data-do-we-work-with",
    "href": "data_management.html#what-data-do-we-work-with",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "What data do we work with?",
    "text": "What data do we work with?\n\nBioinformatics is an interdisciplinary field of science that develops methods and software tools for understanding biological data, especially when the data sets are large and complex. (Wikipedia)\n\nThis data can come from a variety of different biological processes:\n\n\n\nsource: Lizel Potgieter\n\n\nEarly on, sequencing data was not readily available, but due to decreasing costs and increased computational power biological data is now being produced in ever increasing quantities:\n\n\n\nGrowth of the Sequence Read Archive, SRA, from 2012 to 2021\n\n\nAt the same time, new technologies are being developed, and new tools that might or might not be maintained or benchmarked against existing tools. It’s the wild west out there!\n\n\n\nOverview of modern sequencing technologies and where they apply to biological processes",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#working-with-data",
    "href": "data_management.html#working-with-data",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Working with data",
    "text": "Working with data\nOften, with a new project, one sits down with the data, tries out things and see if they worked. A lot of bioinformatics is not being afraid to try things, and reading the documentation.\nThis traditional way of working with bioinformatics data can have merits and lead to new discoveries. However, in this course we would like to introduce you to a more structured way to make sense of your data.\nLet’s have a look at a typical PhD student’s research project:\n\nThey might analyse their data, and get some results.\nAfter talking with their supervisor they might get a few other samples from a collaborator, or need to drop them from the analyses due to quality concerns.\nThey run the analyses again and get a different set of results.\nThere might be a few iterations of this process, and then the reviewers require some additional analyses…\n\nIn the “end” we have something like this:\n\n\n\nWhich one of these is the latest version?\n\n\n\n\n\n\n\n\nTipBest practices file organization\n\n\n\n\nThere is a folder for the raw data, which does not get altered.\nCode is kept separate from data.\nUse a version control system (at least for code) – e.g. git.\nThere should be a README in every directory, describing the purpose of the directory and its contents.\nUse file naming schemes that makes it easy to find files and understand what they are (for humans and machines) and document them.\nUse non-proprietary formats – .csv rather than .xlsx",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#literate-programming",
    "href": "data_management.html#literate-programming",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Literate programming",
    "text": "Literate programming\nOur hypothetical PhD student, even if taking into account the best practice tips from above, is still likely to run the same analyses over and over whenever the input data changes. Sometimes, this might be months, or even years, after the original analysis was performed.\nLuckily, they can save their code snippets (with intuitive file names) and re-use the code from back then. This is often done with R-scripts, but can just as well be applied to bash scripts, python scripts etc.\nIn the past years, the development went even further and one can even combine code and documentation in the same document. The code is wrapped in so called chunks, or code cells, that are executable from within the document.\n\nDebugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it. Brian Kernighan\n\nBefore the course you have already worked with one such notebook - Quarto. We will continue to work with it during this course.\n\n\n\n\n\n\nTip\n\n\n\n\nDocument your methods and workflows.\nDocument where and when you downloaded data.\nDocument the versions of the software that you ran.",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#version-control",
    "href": "data_management.html#version-control",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Version control",
    "text": "Version control\nNow that our student has reproducible documents, with reasonable names, that can execute their analyses reliably over and over again, what happens if they modify their analyses? Will they end up again with different result files and their project sink down in chaos?\nNo, because there is version control, the practice of tracking and managing changes to files.\nAgain, before the course you worked through the basics of git, and how to use it with GitHub collaboratively. We will continue using git during the course as well.",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#environment-managers",
    "href": "data_management.html#environment-managers",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Environment managers",
    "text": "Environment managers\nUsing git, our PhD student can now share their reproducible code with their colloaborators, or between systems. They can rest assured that the different versions of the notebook are tracked and can be checked out when necessary. But what about the bioinformatic tools? Can they also be shared easily?\nDifferent computers can run on different operating systems, or can have different versions of databases installed. This can lead to conflicts between tools, or software versions and can impact code usability, or reproducibility.\nFortunately, smart people have developed environment managers such as conda, bioconda, or pixi. These tools find and install packages, so that the same package versions are being run between different computers. However, the code might still give different results on different operating systems.\nDuring this course we will be building our own environments with Pixi - you’ll see how great it is not having to manually install tools anymore!",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#containers-in-bioinformatics",
    "href": "data_management.html#containers-in-bioinformatics",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Containers in bioinformatics",
    "text": "Containers in bioinformatics\nBut what if our PhD student needs to run their code on different operating systems?\nThey can use containers, that contain everything needed to run the application, even the operating system. Containers are being exchanged as container images, which makes them lightweight. Containers do not change over time, so the results will be the same today and in a few years. Everyone gets the same container that works in the same way.\nIn this course, you will have guessed it, we will learn about containers, where to get them and how to use them.",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#workflow-manager",
    "href": "data_management.html#workflow-manager",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Workflow manager",
    "text": "Workflow manager\nNow our PhD student can use containers, or environments, to provide a uniform environment for their version controlled, wonderfully documented and reproducible code. Fantastic! But they still have to deploy, or at least monitor, their scripts manually.\nFortunately there are workflow managers that can integrate all of the above, submit your jobs for you, and even monitor and re-submit scripts after failure. They will automatically submit jobs for you, decreasing downtime and increasing efficiency.\n\n\n\n\n\n\nTip\n\n\n\nHumans tend to do mistakes, especially when it comes to tedious or repetitive tasks. If you automate data handling, formatting etc. you are less likely to make mistakes like typos, or changing colors in images.\n\n\nIn this course, we will also cover a workflow manager, Nextflow and learn how to make our own workflow, and how to run already developed workflows.",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#goal-of-the-course",
    "href": "data_management.html#goal-of-the-course",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Goal of the course",
    "text": "Goal of the course\nWith this, we want to give you tools that will help you plan and carry out your research. These tools will make your work more efficient and more reproducible. No matter what kind of data you use, you will take something useful from this course.\n\n\n\nOverview of modern sequencing technologies and where they apply to biological processes",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "emPLANT Bioinformatics Course",
    "section": "",
    "text": "This material is part of the emPLANT Bioinformatics Course.\nemPLANT is an international Master’s programme that trains students in modern plant breeding and innovative plant sciences, combining academic excellence with practical skills for research and industry.\nSee the course schedule here.",
    "crumbs": [
      "Reproducibility",
      "Home"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]