[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "emPLANT Bioinformatics Course",
    "section": "",
    "text": "This material is part of the emPLANT Bioinformatics Course.\nemPLANT is an international Master’s programme that trains students in modern plant breeding and innovative plant sciences, combining academic excellence with practical skills for research and industry.\nSee the course schedule here.\nIn this module we will talk about reproducibility in bioinformatics analyses, which will briefly introduce you to some techniques and tools to make your life easier.\nThen we will take a step back and look at how we can assess the quality of (short read) sequencing data. An important step that lays the groundwork to everything else that you will to with your data!",
    "crumbs": [
      "Reproducibility",
      "About the course"
    ]
  },
  {
    "objectID": "qc.html",
    "href": "qc.html",
    "title": "Quality Control of Sequencing Data",
    "section": "",
    "text": "By the end of this module, you should be able to:",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#from-plant-tissue-to-sequencing-reads",
    "href": "qc.html#from-plant-tissue-to-sequencing-reads",
    "title": "Quality Control of Sequencing Data",
    "section": "From plant tissue to sequencing reads",
    "text": "From plant tissue to sequencing reads\nBefore we talk about quality control, it is important to understand how sequencing data is produced. Many of the problems we detect during QC originate early in the sequencing workflow.\nAt a high level, the process looks like this:\n\nPlant tissue → DNA extraction → Library preparation → Sequencing → FASTQ files\n\nEach step can introduce technical effects that later appear in quality control reports. Let’s have a closer look:",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#why-do-we-need-quality-control",
    "href": "qc.html#why-do-we-need-quality-control",
    "title": "Quality Control of Sequencing Data",
    "section": "Why do we need quality control?",
    "text": "Why do we need quality control?\nWhen we work with sequencing data, we usually want to answer a biological question.\nIn plant breeding, these could for example be:\n\nWhich genetic variants are associated with a certain trait?\nWhich alleles are present in a breeding line?\nHow similar are two plant populations?\nWhich genes are expressed under certain conditions?\n\nAll of these questions rely on the assumption that the sequencing data reflects the real biology of the plant.\nQuality control (QC) is the step where we check whether this assumption is reasonable.\nAs such, QC is not about making the data “look nice”. It is about making sure that technical problems do not lead to wrong biological conclusions.",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#what-can-go-wrong-if-data-quality-is-poor",
    "href": "qc.html#what-can-go-wrong-if-data-quality-is-poor",
    "title": "Quality Control of Sequencing Data",
    "section": "What can go wrong if data quality is poor?",
    "text": "What can go wrong if data quality is poor?\nSequencing errors do not stay “technical”. They can directly affect how we interpret the biology:\n\n\n\n\n\n\n\nTechnical issue\nBiological consequence\n\n\n\n\nLow base quality\nIdentification of false SNPs\n\n\nAdapter contamination\nDetection of artificial variants\n\n\nGC bias\nSkewed allele frequency estimates\n\n\nHigh duplication\nOverconfidence in genotypes\n\n\n\n In a breeding context, this can mean that we might choose the wrong markers, miss true marker-trait associations or misjudgement or genetiv diversity.\nQuality control helps us detect these problems before we trust the results.",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#where-do-sequencing-errors-come-from",
    "href": "qc.html#where-do-sequencing-errors-come-from",
    "title": "Quality Control of Sequencing Data",
    "section": "Where do sequencing errors come from?",
    "text": "Where do sequencing errors come from?\nSequencing errors can be introduced at several steps, long before we ever see a FASTQ file.\n\nStep 1: DNA extraction\n\nPlant material can be challenging to work with. Different issues can affect the quality of the reads that will be sequenced:\n\nCompounds such as polyphenols and polysaccharides can interfere with enzymes or make the DNA sticky.\nDNA quality can vary strongly between samples, depending on the extraction method, tissue and extractor.\n\n\n\nStep 2: Library preparation\n\nDuring library preparation, the DNA is sheared into fragments, adapters are ligated to these DNA fragments and the DNA is often amplified using PCR.\nSometimes, the DNA already degrades before the library preparation. That can lead to uneven sequencing coverage in the sequenced reads. The PCR can introduce a bias, where some fragments are amplified more than others.\nAll these effects are technical, but they shape the data we later analyse.\n\n\nSequencing\n\nSequencing run itself can introduce errors. Base calling becomes less accurate toward the end of reads and errors accumulate across sequencing cycles. Different lanes or runs can have different overall quality.",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#why-plant-genomes-are-special",
    "href": "qc.html#why-plant-genomes-are-special",
    "title": "Quality Control of Sequencing Data",
    "section": "Why plant genomes are special",
    "text": "Why plant genomes are special\nMany QC tools are generic. They do not “know” that your data comes from plants.\nHowever, plant genomes often have features that affect quality:\n\nLarge genome sizes\nHigh repeat content\nPolyploidy\nHigh heterozygosity\nVariable GC content between species\n\nAs a result, the GC content plots may look unusual even for good data. High duplication levels are not always a technical problem. So, some warnings are expected and acceptable.\nThis means that QC results must always be interpreted in a biological context.\nQuality control tools can show you what looks unusual — you need biological knowledge to decide why.",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#what-is-fastqc",
    "href": "qc.html#what-is-fastqc",
    "title": "Quality Control of Sequencing Data",
    "section": "What is FastQC?",
    "text": "What is FastQC?\nAfter sequencing, the first files we usually receive are FASTQ files. These files contain millions of short sequencing reads together with a quality score for every base.\nLooking directly at raw FASTQ files is not practical.\nFastQC is a tool for quality control on raw sequence data coming from high throughput sequencing. It takes your raw data and performs a series of standardized tests, and produces an easy-to-read overview report with figures and tables for each sample.\nFastQC provides information about your samples, it does not know what experiment you performed, or what you plan to do with your data afterwards. Thus, the interpretation and decisions are up to you. After checking the quality you might want to use other tools to correct errors, remove low-quality data, etc.",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#interpreting-fastqc-flags",
    "href": "qc.html#interpreting-fastqc-flags",
    "title": "Quality Control of Sequencing Data",
    "section": "Interpreting FastQC flags",
    "text": "Interpreting FastQC flags\nEach FastQC module ends with a simple label: pass, warning, or fail. These labels are often misunderstood. A fail does not mean the data is unusable. Alternatively, a pass does not mean the data is perfect.\nThe flags are based on generic thresholds that work reasonably well across many datasets, but they are not tailored to specific organisms or analyses. In plant breeding projects, it is common to see warnings or failures even in perfectly usable datasets.\nFastQC flags should therefore be treated as signals to investigate, not as automatic decisions.",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#a-fastqc-report",
    "href": "qc.html#a-fastqc-report",
    "title": "Quality Control of Sequencing Data",
    "section": "A FastQC report",
    "text": "A FastQC report\nThe following is adapted from the excellent FastQC tutorial at Babraham Bioinformatics.\n\nBasic statistics\n\nSimple summary statistics for the file that was analyzed. Good to check what sample was used, and to get initial stats on the sample. This section will never raise warnings.\n\n\nPer base sequence quality\n\nThis plot shows the quality score at each position in the read. The higher the score, the better the base call.\nThis plot answers a simple question: How confident are we in the base calls along the read?\nIn most sequencing datasets, quality is highest at the beginning of the read and decreases toward the end. This pattern is normal and expected, a modest drop in quality at the end of reads is usually not a serious problem.\nHowever, the sample can be problematic when:\n\nQuality drops very sharply.\nLarge parts of the read have very low scores.\nQuality varies strongly between samples.\n\n\n\nPer Sequence Quality Scores\n\nWhile the previous plot looks at quality per position, this plot looks at quality per read.\nIt allows you to see if a subset of your sequences have low quality values, or if the quality is homogeneous across the reads of your sample.\nThe low quality reads should obviously only represent a small percentage of the total sequences. If a significant proportion of the sequences are of low quality there could be some kind of systematic problem.\nIn plant breeding data, poor per-sequence quality can indicate issues such as:\n\nProblems during library preparation.\nTechnical issues during sequencing.\nLow-quality starting material.\n\n\n\nPer Base Sequence Content\n\nThis module shows the proportion of the bases - A, C, G, and T - at each position in the read. For random genomic DNA, these proportions should be relatively stable across the read length.\nSome types of library will always produce biased sequence composition, normally at the start of the read. Not every deviation from equal base content is a problem. The key question is whether the pattern is expected given the experiment.\n\n\nPer Sequence GC Content\n\nThe GC content plot shows the overall GC distribution of reads in the sample. FastQC compares the observed distribution to a theoretical expectation, and can help identify contamination.\nThe expectation is a roughly normal distribution of GC content where the peak represents the overall GC content of the underlying genome. However, the acutal shape of the curve depends highly on your sample.\nIn plant datasets, unusual GC content does not automatically mean contamination, it can vary naturally between regions, which can influence this plot.\nUnexpected GC content can be caused by:\n\nGenome composition\nRepetitive regions\nTargeted sequencing approaches\n\nThis plot becomes more informative when comparing samples to each other, rather than judging a single sample in isolation.\n\n\nPer Base N Content\n\nIf a sequencer is unable to make a base call with sufficient confidence it will call an “N” rather than a conventional base.\nIt is not unusual to see a very low proportion of Ns appearing in a sequence, especially closer to the end of a sequence.\n\n\nSequence length distribution\n\nA graph showing the distribution of fragment sizes in the file which was analysed. Here, you can see a sample with many short, and a few very long reads.\nSome high throughput sequencers generate sequence fragments of uniform length (Illumina), but others can contain reads of varying lengths (PacBio, NanoPore).\n\n\nSequence Duplication Levels\n\nThis plot shows how often identical sequences occur in the data. In a diverse library most sequences will occur only once in the final set, so most sequences should fall into the far left of the plot\nHigh duplication can arise from:\n\nPCR amplification bias.\nLow library complexity.\nVery deep sequencing.\n\nIn plant breeding data, duplication is not always technical. Highly conserved or repetitive regions can also produce high duplication levels.\nInterpretation depends strongly on the type of experiment and the genome being studied.\n\n\nOverrepresented sequences\n\nFastQC also identifies sequences that occur much more often than expected. A normal high-throughput library will contain a diverse set of sequences, with no individual sequence making up a substantial fraction of the whole.\nOverrepresented sequences might be biologically significant, or indicate that the library is contaminated, or simply not as diverse as expected. They can be adapter fragments, primers or highly repetitive genomic regions.\nThis module is often useful for diagnosing specific technical issues, especially when combined with adapter content and duplication plots.\n\n\noptional: Kmer content\n\nThe k-mer content module looks for short sequence motifs that occur more often than expected in the reads.\n\n\n\n\n\n\nNote\n\n\n\nA k-mer is simply a short DNA sequence of length k.\nFor example, if k = 7, a k-mer could be something like ATCGTGA. Fixed size kmers can also be called after their number: 3-mer, 7-mer etc.\n\n\nFastQC scans the reads and checks whether certain k-mers are strongly enriched at specific positions along the read. In random genomic sequencing data, short sequence motifs should be more or less evenly distributed, so that is the basic assumption of FastQC.\nThis module measures the number of each 5-mer (default, but adjustable) at each position in your library and then looks for significant deviations from an even coverage at all positions. Any k-mers with positionally biased enrichment are reported. The top 6 most biased Kmer are additionally plotted to show their distribution.\nWhen FastQC detects a strong enrichment of specific k-mers, this often indicates that something systematic is present in the data. Enrichment at the start or end of reads is especially common and often points to technical sequences rather than biological signal.\nCommon causes include:\n\nAdapter or primer sequences.\nLibrary preparation artefacts.\nTechnical biases introduced during sequencing.\n\n\n\n\n\n\n\nNote\n\n\n\nTo save resources and time, only a random 2% of the sample is analysed. Results are then extrapolated to the rest of the library.\n\n\n\n\nAdapter content\n\nThis module will detect adapter content and plot it together with the identity of the adapter.\nGood to know if you have adapter contamination, and which ones you might have to remove from the data.",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#how-fastqc-fits-into-a-larger-workflow",
    "href": "qc.html#how-fastqc-fits-into-a-larger-workflow",
    "title": "Quality Control of Sequencing Data",
    "section": "How FastQC fits into a larger workflow",
    "text": "How FastQC fits into a larger workflow\nFastQC is usually used at least twice:\n\nOnce on raw data\nAgain after trimming or filtering\n\nComparing FastQC reports before and after processing helps you evaluate whether your actions improved the data.\nFastQC is therefore not a one-time checkpoint, but part of an iterative process.",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#your-turn-a-real-fastqc-report",
    "href": "qc.html#your-turn-a-real-fastqc-report",
    "title": "Quality Control of Sequencing Data",
    "section": "Your turn: a real FastQC report",
    "text": "Your turn: a real FastQC report\nSo, now let’s have a look at some actual FastQC reports. Pick one of the below reports, take 20 minutes (with a partner if you have one at hand) and try to understand what is going on. Keep in mind what sort of data you are working with.\n\n\n\n\n\n\nNote\n\n\n\nRemember, most often, there is no single “correct” answer. What matters is whether your interpretation is biologically and technically justified.\n\n\nHere are some guiding questions for you:\n\nLooking at the summary table (in the left sidebar), which modules are marked as warnings or failures? Which ones would you look at first, and why?\nHow does read quality change along the length of the reads? Is the quality along the sample problematic, or acceptable?\nIs there evidence of adapter contamination or overrepresented sequences?\nBased on this report, would you: i) Use the data as is? ii) Apply trimming or filtering? iii) Be concerned about the sample and investigate further?\nAre there any FastQC warnings you would ignore for this dataset? Why are they acceptable in this context?\nIf you had three samples and only this one looked different, what would you want to check next?\n\nReport 1: RNA-sequencing data short read data - remember that RNA sequencing does not look at the whole genome.\nReport 2: bad sample data short read data - can you figure out what happened to this sample?\nHere is a very nice and thorough tutorial on assessing quality of short and long read sequencing data. Great to have a look at the difference the sequencing technology can make!",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#after-quality-control-what-comes-next",
    "href": "qc.html#after-quality-control-what-comes-next",
    "title": "Quality Control of Sequencing Data",
    "section": "After quality control: what comes next?",
    "text": "After quality control: what comes next?\nAfter working through real FastQC reports, one important point should be clear: Quality control is not about deciding whether data is “good” or “bad”. It is about deciding what to do next.\nSequencing data almost always contains imperfections. The goal of QC is to understand whether these imperfections matter for your biological question, and how they should be handled.",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#acting-on-qc-results",
    "href": "qc.html#acting-on-qc-results",
    "title": "Quality Control of Sequencing Data",
    "section": "Acting on QC results",
    "text": "Acting on QC results\nOnce you have interpreted a FastQC report, there are usually three possible outcomes.\n\nIn some cases, the data can be used as it is. Minor warnings or expected patterns do not necessarily require any action, especially if they are unlikely to affect downstream analyses.\nIn other cases, simple processing steps are appropriate. This often includes trimming adapters or low-quality read ends. After such processing, QC should be repeated to confirm that the data has improved.\nIn a small number of cases, QC reveals serious problems. These may require investigating sample preparation, resequencing, or excluding a sample from further analysis.\n\nAt the beginning it might be a lot to go through the reports and judge the samples, but with time and practice it will come more natural to you.",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#qc-as-part-of-a-reproducible-workflow",
    "href": "qc.html#qc-as-part-of-a-reproducible-workflow",
    "title": "Quality Control of Sequencing Data",
    "section": "QC as part of a reproducible workflow",
    "text": "QC as part of a reproducible workflow\nQuality control should not be an informal or invisible step.\nGood practice includes running QC in a consistent way across all samples, recording which issues were observed and documenting which actions were taken, and why.\nThis is especially important in plant breeding projects, where results may influence long-term decisions.\nClear QC reporting makes analyses more transparent and easier to reproduce.",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "qc.html#bonus-multiqc",
    "href": "qc.html#bonus-multiqc",
    "title": "Quality Control of Sequencing Data",
    "section": "Bonus: MultiQC",
    "text": "Bonus: MultiQC\nSo far, we have focused on one FastQC report at a time. In real projects, especially in plant breeding, we usually work with many samples.\nWhen dozens or hundreds of samples are sequenced, it becomes tedious to interpret QC results sample by sample. This is where MultiQC is useful because is collects reports from many samples and condenses them into a single report.\nThat way you can compare samples side by side, spot outliers quickly, and identify systematic problems across samples.",
    "crumbs": [
      "Reproducibility",
      "Quality control"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "data_management.html#data-life-cycle",
    "href": "data_management.html#data-life-cycle",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Data Life Cycle",
    "text": "Data Life Cycle\nWhen working with any type of data, it makes sense to sit down before the project starts to think through the different life stages of the data in your project. This will help counteract some of the problems that can arise when projects grow more organically, and will help consistency within the research group, ease collaboration, and mostly your future self that will understand what past-self has been up to in the project.\n\n\n\n\n\n\nNote\n\n\n\nMore and more funding agencies expect a Data Management Plan at some point of a project application. In there, you need to document that you have thought of, and planned for, the life cycle of your data.\n\n\n\n\n\nThe Research Data Management toolkit for Life Sciences\n\n\nThinking about the data life cycle early helps avoid many common problems later — especially when projects grow, collaborators join, or analyses need to be repeated.",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#fair-principles",
    "href": "data_management.html#fair-principles",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "FAIR principles",
    "text": "FAIR principles\nIn the past, research data was often generated with one question in mind. Often, they would afterwards land in some drawer and be forgotten about. Nowadays researchers acknowledge that data can also be re-used, or combined with other data, to answer different questions.\nThe FAIR principles promote efficient data discovery and reuse by providing guidelines to make digital resources:\n\n\n\nWilkinson et al. (2016)\n\n\n\nFAIR in practice\nFAIR does not mean “open” or “perfect”. It means that someone else (or future-you) can understand and reuse your data with reasonable effort.\nExamples:\n\nFindable\n\nRaw sequencing data deposited in ENA/SRA with a stable accession.\nFiles named in a consistent way (not final_final_v3_really_final.fastq.gz).\n\nAccessible\n\nData stored in a repository that others can access (even if access is controlled).\nClear instructions on how to request access if data is sensitive.\n\nInteroperable\n\nUsing standard formats (FASTQ, BAM, VCF).\nUsing standard metadata (sample IDs, species names, genome versions).\n\nReusable\n\nA README explaining how the data was generated.\nSoftware versions and parameters documented.\n\n\nSo you can see that FAIR principles, in turn, rely on good data management practices in all phases of research:\n\nResearch documentation\nData organisation\nInformation security\nEthics and legislation\n\n\n\nA small FAIR habit: file names\nYou will not make your data or analyses fully fair in one go. And you don’t have to!\nTry implementing one, or a few things, at a time, and once they are part of the way you work try adding something else.\nFile names are often the first piece of metadata someone encounters. They are also one of the easiest things to improve. Remember: File names are not only for computers. They are for humans trying to make sense of a project.\n\n\n\n\n\n\nDiscussion: better file names\n\n\n\nYou receive these three files from a collaborator:\n\nfinal.fastq\ndata_new.fastq\nsampleB.fastq\n\nWhat information is missing? What would you rename them to?",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#reproducible-research",
    "href": "data_management.html#reproducible-research",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Reproducible research",
    "text": "Reproducible research\nLucky for us, once we implement good data management practices, we will also increase the reproducibility of our analyses. That means that analyses can be run again on the same or similar data and get the same results.\n\n\n\n\n\n\nThink about it\n\n\n\nHave you ever:\n\nre-run an analysis and obtained a different result?\nstruggled to understand your own code after some time?\ninherited data or scripts without documentation?\n\nThese are common reproducibility problems.\n\n\nExtensive documentation will increase trust in the outcome of analyses, and will help people (and again, future-you) understand what was done, and why.\nReproducibility is also important for practical reasons. Research projects often change over time: new samples are added, low-quality samples are removed, or reviewers ask for additional analyses. If analyses are reproducible, these changes are much easier to handle.\nLast, but not least, reproducible research practices make project hand-overs smoother. When someone new joins a project, they can rely on clear structure and good documentation, instead of starting from scratch.\n\n\n\n\n\n\nNote\n\n\n\nAnd one more time: reproducible research is not about perfection. It is about making your work understandable and repeatable. Small improvements already make a big difference.",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#what-data-do-we-work-with",
    "href": "data_management.html#what-data-do-we-work-with",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "What data do we work with?",
    "text": "What data do we work with?\n\nBioinformatics is an interdisciplinary field of science that develops methods and software tools for understanding biological data, especially when the data sets are large and complex. (Wikipedia)\n\nThis data can come from a variety of different biological processes:\n\n\n\nsource: Lizel Potgieter\n\n\nEarly on, sequencing data was not readily available, but due to decreasing costs and increased computational power biological data is now being produced in ever increasing quantities:\n\n\n\nGrowth of the Sequence Read Archive, SRA, from 2012 to 2021\n\n\nAt the same time, new technologies are being developed, and new tools that might or might not be maintained or benchmarked against existing tools. It’s the wild west out there!\n\n\n\nOverview of modern sequencing technologies and where they apply to biological processes",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#working-with-data",
    "href": "data_management.html#working-with-data",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Working with data",
    "text": "Working with data\nOften, with a new project, one sits down with the data, tries out things and see if they worked. A lot of bioinformatics is not being afraid to try things, and reading the documentation.\nThis traditional way of working with bioinformatics data can have merits and lead to new discoveries. However, in this course we would like to introduce you to a more structured way to make sense of your data.\nLet’s have a look at a typical PhD student’s research project:\n\nThey might analyse their data, and get some results.\nAfter talking with their supervisor they might get a few other samples from a collaborator, or need to drop them from the analyses due to quality concerns.\nThey run the analyses again and get a different set of results.\nThere might be a few iterations of this process, and then the reviewers require some additional analyses…\n\nIn the “end” we have something like this:\n\n\n\nWhich one of these is the latest version?\n\n\nThis kind of chaos is rarely due to bad intentions or lack of skill. It usually happens because projects evolve, data changes, and analyses are repeated without a clear structure.\n\n\n\n\n\n\nBest practices file organization\n\n\n\n\nThere is a folder for the raw data, which does not get altered.\nCode is kept separate from data.\nUse a version control system (at least for code) – e.g. git.\nThere should be a README in every directory, describing the purpose of the directory and its contents.\nUse file naming schemes that makes it easy to find files and understand what they are (for humans and machines) and document them.\nUse non-proprietary formats – .csv rather than .xlsx\nSeparate files input, intermediate, and final output files go in different directories.\nAvoid manual changes to result files, use (semi-)automated scripts instead.",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#literate-programming",
    "href": "data_management.html#literate-programming",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Literate programming",
    "text": "Literate programming\nEven with good file organisation, our hypothetical PhD student will still run the same analyses over and over whenever the input data changes. Sometimes, this might be months, or even years, after the original analysis was performed.\nOne solution is to save analysis code in scripts (for example R, Python, or bash), using clear file names and comments. This already improves reproducibility compared to running commands manually.\nHowever, scripts and documentation are often stored in separate places, the code in one file, the notes about the analysis in another document, and results somewhere else. This makes it harder to understand how everything fits together.\nLiterate programming is an approach where code and explanation are written together in the same document. The idea is that a human should be able to read the document from top to bottom and understand what the analysis is doing and why they were chosen.\nThe code is placed in special blocks (often called code chunks or code cells) that can be run directly from within the document.\n\nDebugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it.\n— Brian Kernighan\n\nThere are several tools that support literate programming. Examples include Jupyter Notebooks, R Markdown, and Quarto.\nQuarto is very close to my heart, it’s easy and very versatile. I have, for example, used it to make this homepage, for example. But for now, we will focus on the underlying idea using plain Markdown.\n\n\n\n\n\n\nThink like a reader\n\n\n\nImagine you open an analysis document written by someone else.\nWhat would you want to know:\n\nbefore running the analysis?\nwhile reading the code?\nwhen interpreting the results?\n\nThese are exactly the things literate programming tries to capture.",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#version-control",
    "href": "data_management.html#version-control",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Version control",
    "text": "Version control\nEven with well-organised files and clearly documented analyses, our hypothetical PhD student’s project continues to evolve over time. They might fix a small mistake they caught somewhere, adjust parameters after discussing with their supervisor, or extend the analyses after reviewer comments.\nA very common way of handling this is to create multiple copies of the same file, often with only slightly different names. While this feels safe, it quickly becomes confusing. It is hard to know which file was used to generate a specific result, and easy to accidentally work on the wrong version.\nTo keep track of what changed, when, and why, our student can use version control.\nThere are several version control tools that can help with tracking files. One very popular one is git. Git keeps a history of changes to files, rather than creating multiple copies of them.\nIt can track changes to text files and works best for text-based files, such as scripts, Markdown documents, and configuration files.\n\n\n\n\n\n\nNote\n\n\n\nLarge raw data files (for example sequencing data) are usually not tracked with git. Instead, these files are stored separately, while git tracks the code and documentation that describe how the data was processed.\n\n\nVersion control also helps when working with other people. Several people can work on the same project without overwriting each other’s work, and it is possible to return to an earlier state if something goes wrong.\nSo, version control can help us manage changes to code and documentation. However, reproducibility also depends on having the same software available across systems.",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#environment-managers",
    "href": "data_management.html#environment-managers",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Environment managers",
    "text": "Environment managers\nSo far, our hypothetical PhD student has organised their files, documented their analyses, and started using version control to track changes to code and documents.\nHowever, they now run into a new problem: The same analysis can behave differently on different computers, code that worked perfectly on one machine may fail or behave differently on another.\nDifferent computers can run on different operating systems, or can have different versions of databases installed. This can lead to conflicts between tools, or software versions and can impact code usability, or reproducibility.\nOne way to address this problem is to clearly define the software environment used for an analysis.\nA software environment describes which tools and versions are used and which libraries they depend on.\nEnvironment managers are tools that help create and manage such software environments.\nThey automatically find and install the correct versions of tools and libraries, making it much easier to run the same analysis on different computers.\nThere are several environment managers commonly used in bioinformatics, for example: Conda, Bioconda, or Pixi.\n\n\n\n\n\n\nNote\n\n\n\nToday, we will not be using environment managers, but I hope you remember them when the time comes. It’s great to not have to manually install tools anymore!\n\n\nBy using an environment manager, we can be confident that:\n\nthe same tools are used across machines,\ncollaborators can recreate the environment,\nand analyses are less likely to break due to software differences.",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#containers-in-bioinformatics",
    "href": "data_management.html#containers-in-bioinformatics",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Containers in bioinformatics",
    "text": "Containers in bioinformatics\nEnvironment managers help our PhD student ensure that the same software versions are used across different computers.\nHowever, there are situations where this is not enough.\nFor example, the student might need to:\n\nrun their analysis on a high-performance computing cluster,\nshare the analysis with collaborators using a different operating system,\nor rerun the analysis several years later on a new system.\n\nIn these cases, differences between operating systems can still cause problems, even if the same software versions are used.\nContainers provide a way to package everything needed to run an analysis in one place. They contain the software tools, their dependencies and even the operating system. This means that the analysis runs in the same environment, regardless of where the container is used.\nContainers are shared as container images. An image is a fixed, read-only snapshot of the environment. Because container images do not change over time, they help ensure that analyses produce the same results today and in the future. Think of a container as a self-contained box that runs the same way everywhere.\nCommon container technologies in bioinformatics include:\n\nDocker\nSingularity (also known as Apptainer)\n\n\n\n\n\n\n\nNote\n\n\n\nWe will not work with containers today, but remember that they are a powerful tool that can help you scale your analyses effectively.",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#workflow-manager",
    "href": "data_management.html#workflow-manager",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Workflow manager",
    "text": "Workflow manager\nAt this point, our exemplary PhD student has:\n\nwell-organised files,\ndocumented analyses,\nversion-controlled code,\nand a consistent software setup using environments or containers.\n\nThis already improves reproducibility a lot. However, running analyses is still largely a manual process: running scripts in the correct order and monitoring them for execution of failure.\nManual coordination takes time and concentration. It also increases the risk of small mistakes, especially when analyses are complex or need to be repeated many times.\nWorkflow managers are tools that describe an analysis as a set of connected steps.\nEach step takes defined input files, performs a specific task, and produces defined output files.\nThe workflow manager then takes care of running these steps in the correct order. This makes analyses more efficient and less error-prone.\n\n\n\n\n\n\nTip\n\n\n\nHumans tend to do mistakes, especially when it comes to tedious or repetitive tasks. If you automate data handling, formatting etc. you are less likely to make mistakes like typos, or changing colors in images.\n\n\nThere are several workflow managers used in bioinformatics. One widely used example is Nextflow.\nBy using Nextflow, our student no longer has to remember which commands to run and in which order. Instead, the workflow itself becomes a clear, reproducible description of the analysis.",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#goal-of-the-course",
    "href": "data_management.html#goal-of-the-course",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Goal of the course",
    "text": "Goal of the course\nWith this, I wanted to give you ideas and tools that will help you plan and carry out your research. These tools will make your work more efficient and more reproducible. No matter what kind of data you will work with, I hope you will take something useful from this course.\n\n\n\nOverview of modern sequencing technologies and where they apply to biological processes",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "sequencing_closeread.html",
    "href": "sequencing_closeread.html",
    "title": "EMPLANT course",
    "section": "",
    "text": "We start with plant tissue and extract the contained DNA.\n\n\n\n\nNext, sequencing machines do not read whole chromosomes. Instead, DNA is broken into many small fragments, the so called DNA library.\n\n\n\n\nThe DNA library (with adapters added), is then run on a sequencing machine, which calls the bases of each fragment.\n\n\n\n\nThe final output are usually FASTQ files, which contain raw reads and quality scores."
  },
  {
    "objectID": "closeread_secion/sequencing_closeread.html",
    "href": "closeread_secion/sequencing_closeread.html",
    "title": "EMPLANT course",
    "section": "",
    "text": "We start with plant tissue and extract the contained DNA.\n\n\n\n\nNext, sequencing machines do not read whole chromosomes. Instead, DNA is broken into many small fragments, the so called DNA library.\n\n\n\n\nThe DNA library (with adapters added), is then run on a sequencing machine, which calls the bases of each fragment.\n\n\n\n\nThe final output are usually FASTQ files, which contain raw reads and quality scores."
  },
  {
    "objectID": "closeread_section/sequencing_closeread.html",
    "href": "closeread_section/sequencing_closeread.html",
    "title": "EMPLANT course",
    "section": "",
    "text": "We start with plant tissue and extract the contained DNA.\n\n\n\n\nNext, sequencing machines do not read whole chromosomes. Instead, DNA is broken into many small fragments, the so called DNA library.\n\n\n\n\nThe DNA library (with adapters added), is then run on a sequencing machine, which calls the bases of each fragment.\n\n\n\n\nThe final output are usually FASTQ files, which contain raw reads and quality scores."
  },
  {
    "objectID": "data_management.html#practical-trying-literate-programming-with-markdown-1015-min",
    "href": "data_management.html#practical-trying-literate-programming-with-markdown-1015-min",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Practical: trying literate programming with Markdown (10–15 min)",
    "text": "Practical: trying literate programming with Markdown (10–15 min)\nThis is where your online Markdown renderer shines.\n```markdown ::: {.callout-tip title=“Exercise: a minimal reproducible document”}\nUsing the online Markdown editor, create a short document that includes:\n\nA title\nA short description of where the data came from\nA list of software tools used\nOne code block (it does not need to run)\n\nThink of this as a human-readable analysis report. :::\nLiterate programming helps us keep code and explanation together. But analysis documents change over time.\nTo keep track of what changed and when, we need version control.",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#practical-markdown",
    "href": "data_management.html#practical-markdown",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Practical: Markdown",
    "text": "Practical: Markdown\nWe can try out markdown, the language used in many literate programming notebooks online. Please follow this link to https://markdownlivepreview.com/.\n\n\n\n\n\n\nExercise: a minimal reproducible document\n\n\n\nUsing the online Markdown editor, write a small report on a project you’d like to work on:\n\nIt should have a title\nIt should contain a little bit of text about the project.\nIt should contain a link to a wikipedia page (or any other homepage).\n\nThink of this as a human-readable analysis report.",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#goal-of-today",
    "href": "data_management.html#goal-of-today",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Goal of today",
    "text": "Goal of today\nWith this, I wanted to give you ideas and tools that will help you plan and carry out your research. These tools will make your work more efficient and more reproducible. No matter what kind of data you will work with, I hope you will take something useful from this course and today.",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#goal-for-the-rest-of-this-session",
    "href": "data_management.html#goal-for-the-rest-of-this-session",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Goal for the rest of this session",
    "text": "Goal for the rest of this session\nWith the following, I want to give you ideas and tools that will help you plan and carry out your research.\nWe will not actually train on the tools, but if you spend some time learning them by yourself, I guarentee that they will make your work more efficient (and more reproducible).\nNo matter what kind of data you are working with, I hope you will take something useful from this course and today.",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#putting-it-all-together",
    "href": "data_management.html#putting-it-all-together",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Putting it all together",
    "text": "Putting it all together\nThroughout this module, we followed our hypothetical PhD student as their project grew in complexity.\nAt each stage, new challenges appeared:\n\ndata had to be organised and documented,\nanalyses needed to be rerun and updated,\nsoftware behaved differently across systems,\nand manual execution became error-prone.\n\nEach tool we introduced addresses a specific problem. They are not alternatives to each other, but build on top of one another.\n\n\n\n\n\nflowchart BT\n  R[\"Raw data&lt;br/&gt;&lt;span style='font-size:0.8em'&gt;stored separately, unchanged&lt;/span&gt;\"]:::raw\n  D[\"Code & documentation&lt;br/&gt;&lt;span style='font-size:0.8em'&gt;scripts, Markdown, README&lt;/span&gt;\"]:::code\n  V[\"Version control&lt;br/&gt;&lt;span style='font-size:0.8em'&gt;git&lt;/span&gt;\"]:::vc\n  E[\"Environment manager&lt;br/&gt;&lt;span style='font-size:0.8em'&gt;Pixi, Conda&lt;/span&gt;\"]:::env\n  C[\"Container&lt;br/&gt;&lt;span style='font-size:0.8em'&gt;Docker, Singularity&lt;/span&gt;\"]:::cont\n  W[\"Workflow manager&lt;br/&gt;&lt;span style='font-size:0.8em'&gt;Nextflow&lt;/span&gt;\"]:::wf\n\n  R --&gt; D --&gt; V --&gt; E --&gt; C --&gt; W\n\n  classDef raw  fill:#f9f9f9,stroke:#999,stroke-width:1px,rx:10,ry:10;\n  classDef code fill:#eef4ff,stroke:#6b8dd6,stroke-width:1px,rx:10,ry:10;\n  classDef vc   fill:#ede7f6,stroke:#7e57c2,stroke-width:1px,rx:10,ry:10;\n  classDef env  fill:#e8f6f1,stroke:#4aa382,stroke-width:1px,rx:10,ry:10;\n  classDef cont fill:#fff3e6,stroke:#d19a66,stroke-width:1px,rx:10,ry:10;\n  classDef wf   fill:#f3e8ff,stroke:#8b6fcf,stroke-width:1px,rx:10,ry:10;\n\nlinkStyle default stroke:#666,stroke-width:2.5px\n\n\n\n\n\n\nThe higher we move in the diagram, the more predictable and repeatable our analysis becomes.\nAt the same time, the lower layers remain essential: raw data stays unchanged, and clear code and documentation are always the foundation.\n\n\n\n\n\n\nNote\n\n\n\nYou do not need to use all of these tools for every project. Many analyses only require a few of these layers.\nHowever, understanding how they fit together helps you:\n\nchoose the right tool for the problem at hand,\nrecognise common sources of errors,\nand gradually improve the reproducibility of your work.\n\n\n\nAnd most importanlty, these ideas apply regardless of the type of data or tools you use and they will remain useful throughout your research career.",
    "crumbs": [
      "Reproducibility"
    ]
  },
  {
    "objectID": "data_management.html#putting-it-all-together-1",
    "href": "data_management.html#putting-it-all-together-1",
    "title": "Tips and Tools for Reproducible Bioinformatics",
    "section": "Putting it all together",
    "text": "Putting it all together\nThroughout this module, we followed our hypothetical PhD student as their project grew in complexity.\nAt each stage, new challenges appeared: - data had to be organised and documented, - analyses needed to be rerun and updated, - software behaved differently across systems, - and manual execution became error-prone.\nEach tool we introduced addresses a specific problem. They are not alternatives to each other, but build on top of one another.\n\n\n\n\n\nflowchart BT\n  R[\"Raw data&lt;br/&gt;(stored separately, unchanged)\"]:::layer\n  D[\"Code + documentation&lt;br/&gt;(scripts, Markdown, README)\"]:::layer\n  E[\"Environment&lt;br/&gt;(e.g. Pixi / Conda)\"]:::layer\n  C[\"Container&lt;br/&gt;(e.g. Docker / Singularity)\"]:::layer\n  W[\"Workflow manager&lt;br/&gt;(e.g. Nextflow)\"]:::layer\n\n  R --&gt; D --&gt; E --&gt; C --&gt; W\n\n  classDef layer fill:#f7f7f7,stroke:#333,stroke-width:1px,rx:8,ry:8;",
    "crumbs": [
      "Reproducibility"
    ]
  }
]