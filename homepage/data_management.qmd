---
title: "Tips and Tools for Reproducible Bioinformatics"
---
<br>

## Data Life Cycle

When working with any type of data, it makes sense to sit down before the project starts to think through the different life stages of the data in your project. This will help counteract some of the problems that can arise when projects grow more organically, and will help consistency within the research group, ease collaboration, and mostly your future self that will understand what past-self has been up to in the project. 

::: {.callout-note}

More and more funding agencies expect a Data Management Plan at some point of a project application. In there, you need to document that you have thought of, and planned for, the life cycle of your data. 
:::

![[The Research Data Management toolkit for Life Sciences](https://rdmkit.elixir-europe.org/)](figures/data_management/Data_life_cycle.png){.class width=50%}

Thinking about the data life cycle early helps avoid many common problems later — especially when projects grow, collaborators join, or analyses need to be repeated.

## FAIR principles

In the past, research **data was often generated with one question in mind**. Often, they would afterwards land in some drawer and be forgotten about. Nowadays researchers acknowledge that **data can also be re-used, or combined with other data, to answer different questions**. 

The FAIR principles promote **efficient data discovery and reuse** by providing guidelines to make digital resources:

![[Wilkinson et al. (2016)](https://www.tpximpact.com/knowledge-hub/insights/fair-data-guide/
)](figures/data_management/fair-data-principles.webp){.class width=50%}

### FAIR in practice

FAIR does **not** mean “open” or “perfect”. It means that *someone else* (or future-you)
can understand and reuse your data with reasonable effort.

Examples:

- **Findable**
  - Raw sequencing data deposited in ENA/SRA with a stable accession.
  - Files named in a consistent way (not `final_final_v3_really_final.fastq.gz`).

- **Accessible**
  - Data stored in a repository that others can access (even if access is controlled).
  - Clear instructions on *how* to request access if data is sensitive.

- **Interoperable**
  - Using standard formats (FASTQ, BAM, VCF).
  - Using standard metadata (sample IDs, species names, genome versions).

- **Reusable**
  - A README explaining how the data was generated.
  - Software versions and parameters documented.


So you can see that FAIR principles, in turn, rely on **good data management practices** in all phases of research:

- Research documentation
- Data organisation
- Information security
- Ethics and legislation

### A small FAIR habit: file names

You will not make your data or analyses fully fair in one go. And you don't have to!

Try implementing one, or a few things, at a time, and once they are part of the way you work try adding something else. 

File names are often the **first piece of metadata** someone encounters. They are also one of the easiest things to improve. Remember: File names are not only for computers.
They are for humans trying to make sense of a project.

::: {.callout-tip title="Discussion: better file names"}

You receive these three files from a collaborator:

- `final.fastq`
- `data_new.fastq`
- `sampleB.fastq`

What information is missing?
What would you rename them to?
:::

## Reproducible research

Lucky for us, once we implement good data management practices, we will also increase the **reproducibility of our analyses**. That means that analyses can be **run again on the same or similar data** and get the **same results**. 

::: {.callout-tip title="Think about it"}
Have you ever:

- re-run an analysis and obtained a different result?
- struggled to understand your own code after some time?
- inherited data or scripts without documentation?

These are common reproducibility problems.
:::

Extensive documentation will increase trust in the outcome of analyses, and will help people (and again, future-you) **understand what was done, and why**. 

Reproducibility is also important for practical reasons. Research projects often change over time: new samples are added, low-quality samples are removed, or reviewers ask for additional analyses. If analyses are reproducible, these changes are much easier to handle.

Last, but not least, reproducible research practices make project hand-overs smoother. When someone new joins a project, they can rely on clear structure and good documentation, instead of starting from scratch.


::: {.callout-note}
And one more time: reproducible research is not about perfection. It is about making your work understandable and repeatable. **Small improvements already make a big difference.**
:::


## What data do we work with?

> Bioinformatics is an interdisciplinary field of science that develops methods and software tools for understanding biological data, especially when the data sets are large and complex.
([Wikipedia](https://en.wikipedia.org/wiki/Bioinformatics))

This data can come from a variety of different biological processes:

![source: Lizel Potgieter](figures/data_management/bioinformatics_dogma.png){.class width=70%}


Early on, sequencing data was not readily available, but due to [decreasing costs](https://www.genome.gov/about-genomics/fact-sheets/Sequencing-Human-Genome-cost) and [increased computational power](https://en.wikipedia.org/wiki/Moore%27s_law) biological data is now being produced in ever increasing quantities:

![[Growth of the Sequence Read Archive, SRA, from 2012 to 2021](https://academic.oup.com/nar/article/50/D1/D387/6438001)](figures/data_management/SRA_available_data.jpg){.class width=70%}


At the same time, new technologies are being developed, and new tools that might or might not be maintained or benchmarked against existing tools. It's the wild west out there!

![Overview of modern sequencing technologies and where they apply to biological processes](figures/data_management/technologies.png){.class width=70%}

## Working with data

Often, with a new project, one sits down with the data, tries out things and see if they worked. A lot of bioinformatics is not being afraid to try things, and reading the documentation. 

This traditional way of working with bioinformatics data can have merits and lead to new discoveries. However, in this course we would like to **introduce you to a more structured way to make sense of your data**. 

Let's have a look at a typical PhD student's research project: 

- They might analyse their data, and get some results. 
- After talking with their supervisor they might get a few other samples from a collaborator, or need to drop them from the analyses due to quality concerns. 
- They run the analyses again and get a different set of results. 
- There might be a few iterations of this process, and then the reviewers require some additional analyses...

In the "end" we have something like this: 

![Which one of these is the latest version?](figures/data_management/data_chaos.png){.class width=20%}

This kind of chaos is rarely due to bad intentions or lack of skill. It usually happens because **projects evolve**, **data changes**, and **analyses are repeated without a clear structure**.

::: {.callout-tip title="Best practices file organization"}
- There is a folder for the raw data, which **does not get altered**.
- Code is **kept separate** from data.
- Use a **version control** system (at least for code) – e.g. git.
- There should be a **README** in every directory, describing the purpose of the directory and its contents.
- Use **file naming** schemes that makes it easy to find files and understand what they are (for humans and machines) and document them.
- Use **non-proprietary formats** – .csv rather than .xlsx
- **Separate files** input, intermediate, and final output files go in different directories.
- **Avoid manual changes** to result files, use (semi-)automated scripts instead. 
:::

## Literate programming

Even with good file organisation, our hypothetical PhD student will still run the same analyses over and over whenever the input data changes. Sometimes, this might be months, or even years, after the original analysis was performed. 

One solution is to save analysis code in scripts (for example R, Python, or bash), using clear file names and comments. This already improves reproducibility compared to running commands manually.

However, **scripts and documentation are often stored in separate places**, the code in one file, the notes about the analysis in another document, and results somewhere else. This makes it harder to understand how everything fits together.

Literate programming is an approach where **code and explanation are written together in the same document**. The idea is that a human should be able to read the document from top to bottom and understand what the analysis is doing and why they were chosen. 

The code is placed in special blocks (often called *code chunks* or *code cells*) that can be run directly from within the document.


> Debugging is twice as hard as writing the code in the first place.
> Therefore, if you write the code as cleverly as possible, you are,
> by definition, not smart enough to debug it.  
> — Brian Kernighan

There are several tools that support literate programming.
Examples include Jupyter Notebooks, R Markdown, and Quarto.

Quarto is very close to my heart, it's easy and very versatile. I have, for example, used it to make this homepage, for example. But for now, we will focus on the underlying idea using plain **Markdown**.

::: {.callout-tip title="Think like a reader"}

Imagine you open an analysis document written by someone else.

What would you want to know:

- before running the analysis?
- while reading the code?
- when interpreting the results?

These are exactly the things literate programming tries to capture.

:::


## Practical: Markdown

We can try out markdown, the language used in many literate programming notebooks online. Please follow this link to [https://markdownlivepreview.com/](https://markdownlivepreview.com/). 

::: {.callout-tip title="Exercise: a minimal reproducible document"}

Using the online Markdown editor, create a short document that includes:

- A title
- A short description of where the data came from
- A list of software tools used
- One code block (it does not need to run)

Think of this as a *human-readable analysis report*.
:::

## Goal for the rest of this session

With the following, I want to give you ideas and tools that will help you plan and carry out your research. 

We will not actually train on the tools, but if you spend some time learning them by yourself, I guarentee that **they will make your work more efficient** (and more reproducible). 

No matter what kind of data you are working with, I hope you will take something useful from this course and today.

<br>
![Overview of modern sequencing technologies and where they apply to biological processes](figures/data_management/technologies.png){.class width=70%}


## Version control

Even with well-organised files and clearly documented analyses, our hypothetical PhD student’s **project continues to evolve over time**. They might fix a small mistake they caught somewhere, adjust parameters after discussing with their supervisor, or extend the analyses after reviewer comments. 

A very common way of handling this is to create multiple copies of the same file, often with only slightly different names. While this feels safe, it **quickly becomes confusing**. It is hard to know which file was used to generate a specific result, and easy to accidentally work on the wrong version.

To keep track of *what* changed, *when*, and *why*, our student can use **version control**.

There are several version control tools that can help with tracking files. One very popular one is **git**. Git keeps a history of changes to files, rather than creating multiple copies of them.

It can track changes to text files and works best for **text-based files**, such as scripts, Markdown documents, and configuration files. 

::: {.callout-note}
**Large raw data files** (for example sequencing data) are usually **not tracked with git**. Instead, these files are stored separately, while git tracks the code and documentation that describe how the data was processed.
:::

Version control also helps when working with other people. Several people can work on the same project without overwriting each other’s work, and it is possible to return to an earlier state if something goes wrong.

So, version control can help us manage changes to code and documentation. However, reproducibility also depends on having the same software available across systems.

## Environment managers

So far, our hypothetical PhD student has organised their files, documented their analyses, and started using version control to track changes to code and documents.

However, they now run into a new problem: The **same analysis can behave differently on different computers**, **code that worked perfectly on one machine may fail or behave differently on another**.

Different computers can run on different operating systems, or can have different versions of databases installed. This can lead to conflicts between tools, or software versions and can impact code usability, or reproducibility. 

One way to address this problem is to clearly define the **software environment** used for an analysis.

A software environment describes which tools and versions are used and which libraries they depend on.

Environment managers are tools that help create and manage such software environments.

They automatically **find and install the correct versions of tools and libraries**, making it much easier to run the same analysis on different computers.

There are several environment managers commonly used in bioinformatics, for example: `Conda`, `Bioconda`, or `Pixi`. 

::: {.callout-note}
Today, we will not be using environment managers, but I hope you remember them when the time comes. It's great to not have to manually install tools anymore!
:::

By using an environment manager, we can be confident that:

- the same tools are used across machines,
- collaborators can recreate the environment,
- and analyses are less likely to break due to software differences.

## Containers in bioinformatics

Environment managers help our PhD student ensure that the same software versions are used across different computers.

However, there are **situations where this is not enough**.

For example, the student might need to:

- run their analysis on a high-performance computing cluster,
- share the analysis with collaborators using a different operating system,
- or rerun the analysis several years later on a new system.

In these cases, **differences between operating systems can still cause problems**, even if the same software versions are used.

Containers provide a way to package everything needed to run an analysis in one place. They **contain the software tools**, their **dependencies** and even the **operating system**. This means that the analysis runs in the same environment, regardless of where the container is used.

Containers are shared as **container images**. An image is a fixed, read-only snapshot of the environment. Because container images do not change over time, they help ensure that analyses produce the same results today and in the future. **Think of a container as a self-contained box that runs the same way everywhere.**

Common container technologies in bioinformatics include:

- Docker
- Singularity (also known as Apptainer)

::: {.callout-note}
We will not work with containers today, but remember that they are a powerful tool that can help you scale your analyses effectively. 
:::

## Workflow manager

At this point, our exemplary PhD student has:

- well-organised files,
- documented analyses,
- version-controlled code,
- and a consistent software setup using environments or containers.

This already improves reproducibility a lot. However, running analyses is still largely a manual process: running scripts in the correct order and monitoring them for execution of failure. 

**Manual coordination takes time and concentration**. It also increases the risk of small mistakes, especially when analyses are complex or need to be repeated many times.

`Workflow managers` are tools that describe an analysis as a **set of connected steps**.

Each step takes defined input files, performs a specific task, and produces defined output files.

The workflow manager then takes care of running these steps in the correct order. **This makes analyses more efficient and less error-prone.**

::: {.callout-tip}
Humans tend to do mistakes, especially when it comes to tedious or repetitive tasks. If you automate data handling, formatting etc. you are less likely to make mistakes like typos, or changing colors in images.
:::

There are several workflow managers used in bioinformatics. One widely used example is **Nextflow**. 

By using Nextflow, our student no longer has to remember which commands to run and in which order. Instead, **the workflow itself becomes a clear, reproducible description of the analysis**.

## Putting it all together

Throughout this module, we followed our hypothetical PhD student as their project
grew in complexity.

At each stage, new challenges appeared:

- data had to be organised and documented,
- analyses needed to be rerun and updated,
- software behaved differently across systems,
- and manual execution became error-prone.

Each tool we introduced addresses a **specific problem**.
They are not alternatives to each other, but **build on top of one another**.

::: {.center}
```{mermaid}
flowchart BT
  R["Raw data<br/><span style='font-size:0.8em'>(stored separately, unchanged)</span>"]:::raw
  D["Code & documentation<br/><span style='font-size:0.8em'>(scripts, Markdown, README)</span>"]:::code
  E["Software environment<br/><span style='font-size:0.8em'>(Pixi, Conda)</span>"]:::env
  C["Container<br/><span style='font-size:0.8em'>(Docker, Singularity)</span>"]:::cont
  W["Workflow manager<br/><span style='font-size:0.8em'>(Nextflow)</span>"]:::wf

  R --> D --> E --> C --> W

  classDef raw  fill:#f9f9f9,stroke:#999,stroke-width:1px,rx:10,ry:10;
  classDef code fill:#eef4ff,stroke:#6b8dd6,stroke-width:1px,rx:10,ry:10;
  classDef env  fill:#e8f6f1,stroke:#4aa382,stroke-width:1px,rx:10,ry:10;
  classDef cont fill:#fff3e6,stroke:#d19a66,stroke-width:1px,rx:10,ry:10;
  classDef wf   fill:#f3e8ff,stroke:#8b6fcf,stroke-width:1px,rx:10,ry:10;
```
:::

The higher we move in the diagram, the **more predictable and repeatable** our analysis becomes.

At the same time, **the lower layers remain essential**:
raw data stays unchanged, and clear code and documentation are always the foundation.

::: {.callout-note}
You **do not need to use all of these tools for every project**.
Many analyses only require a few of these layers.

However, understanding how they fit together helps you:

- choose the right tool for the problem at hand,
- recognise common sources of errors,
- and gradually improve the reproducibility of your work.
:::

And most importanlty, these ideas apply regardless of the type of data or tools you use and they will remain useful throughout your research career.
